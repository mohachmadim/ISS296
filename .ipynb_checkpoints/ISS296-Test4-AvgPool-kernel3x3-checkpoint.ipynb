{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import tflearn.data_utils as du\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.preprocessing.image import load_img \n",
    "from keras.preprocessing.image import img_to_array \n",
    "from keras.preprocessing.image import array_to_img\n",
    "import warnings\n",
    "import matplotlib.image as mpimg\n",
    "import os \n",
    "import cv2\n",
    "import tensorflow as tf \n",
    "from PIL import Image \n",
    "from numpy import asarray\n",
    "from tqdm import tqdm\n",
    "from tkinter import Tk     # from tkinter import Tk for Python 3.x\n",
    "from tkinter.filedialog import askopenfilename\n",
    "from tkinter import messagebox\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 13440/13440 [00:05<00:00, 2475.79it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 3360/3360 [00:01<00:00, 2538.84it/s]\n"
     ]
    }
   ],
   "source": [
    "#we need to convert the images to a matrix so we can build our test and train sets\n",
    "\n",
    "DIRECTORY = r\"C:\\Users\\rassa\\Documents\\Machine Learning A-Z  Udemy\\Arabic Handwritten Characters Dataset\"\n",
    "train_data = []\n",
    "test_data = []\n",
    "trainPath = DIRECTORY + r\"\\Train Images 13440x32x32\\train\"\n",
    "testPath  = DIRECTORY + r\"\\Test Images 3360x32x32\\test\"\n",
    "\n",
    "def getImgId(imgName):\n",
    "    return int(imgName.split('_')[1])\n",
    "\n",
    "def makeData(data, directory, sortingCriteria):\n",
    "    imgFileNameList = os.listdir(directory)     #get list of directories in path (ie. list of image names)\n",
    "    imgFileNameList.sort(key = sortingCriteria) #sort list based on key\n",
    "    \n",
    "    for img in tqdm(imgFileNameList):  # iterate over the sorted list\n",
    "                try:\n",
    "                    img_array = cv2.imread(os.path.join(directory, img) ,cv2.IMREAD_GRAYSCALE)  # convert image to array\n",
    "                    data.append([img_array])  # add this to our data\n",
    "                except Exception as e:  # just in case there is a mistake.\n",
    "                    pass\n",
    "\n",
    "makeData(train_data, trainPath, getImgId)\n",
    "makeData(test_data, testPath, getImgId)\n",
    "\n",
    "# convert to ndarray and reshape\n",
    "train_data = np.asarray(train_data).reshape([-1, 32, 32, 1])\n",
    "test_data = np.asarray(test_data).reshape([-1, 32, 32, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the labels\n",
    "#header = 0 because\n",
    "#this creates a pandas dataframe (basically a table)\n",
    "train_label = pd.read_csv(DIRECTORY + '/csvTrainLabel 13440x1.csv', header = None)\n",
    "test_label = pd.read_csv(DIRECTORY +  '/csvTestLabel 3360x1.csv', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data = train_data.iloc[:,:].values.astype('float32')\n",
    "train_label = train_label.iloc[:,:].values.astype('int32')-1\n",
    "#test_data = test_data.iloc[:,:].values.astype('float32') we don't need these anymore\n",
    "test_label = test_label.iloc[:,:].values.astype('int32')-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = du.to_categorical(train_label,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data/255\n",
    "test_data = test_data/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data, mean1 = du.featurewise_zero_center(train_data)\n",
    "#test_data, mean2 = du.featurewise_zero_center(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "recognizer = Sequential()\n",
    "#for any kind of deep learning we use the sequential model in keras then \n",
    "#add layers to it\n",
    "recognizer.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu', input_shape = (32,32,1)))\n",
    "#the first 2 dimensional convolusion layer will have 32 filters. filters or kernels are \n",
    "#what we use to extract features. in this case they are of size 3x3 (kernel size)\n",
    "#in the first layer it is necessary to specify the shape so for our case its a\n",
    "#32x32 pixel image and since it's black and white then it has only one dimension or channel\n",
    "#if it was colored then we would have (32,32,3)\n",
    "#activation='relu' this is rectified linear unit. the output filters or convolved layers\n",
    "#might contain some negative values so we apply the rectifier function (or other functions) to break linearity\n",
    "recognizer.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "recognizer.add(AveragePooling2D(pool_size=(2,2)))\n",
    "recognizer.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "recognizer.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "recognizer.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "recognizer.add(MaxPool2D(pool_size=(2,2), strides=(1,1)))\n",
    "recognizer.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "recognizer.add(Flatten())\n",
    "recognizer.add(Dense(units = 256, input_dim = 1024, activation = 'relu'))\n",
    "recognizer.add(Dense(units = 256, activation = \"relu\"))\n",
    "recognizer.add(Dropout(0.5))\n",
    "recognizer.add(Dense(28, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_20 (Conv2D)           (None, 32, 32, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "average_pooling2d_9 (Average (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 14400)             0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 256)               3686656   \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 28)                7196      \n",
      "=================================================================\n",
      "Total params: 3,824,636\n",
      "Trainable params: 3,824,636\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "recognizer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "recognizer.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False, \n",
    "        samplewise_center=False,  \n",
    "        featurewise_std_normalization=False,\n",
    "        samplewise_std_normalization=False,\n",
    "        zca_whitening=False,\n",
    "        rotation_range=10,\n",
    "        zoom_range = 0.1,  \n",
    "        width_shift_range=0.1, \n",
    "        height_shift_range=0.1,\n",
    "        horizontal_flip=False,\n",
    "        vertical_flip=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "134/134 - 32s - loss: 2.3766 - acc: 0.2840\n",
      "Epoch 2/30\n",
      "134/134 - 64s - loss: 1.2785 - acc: 0.5776\n",
      "Epoch 3/30\n",
      "134/134 - 89s - loss: 0.9240 - acc: 0.6941\n",
      "Epoch 4/30\n",
      "134/134 - 116s - loss: 0.7211 - acc: 0.7602\n",
      "Epoch 5/30\n",
      "134/134 - 117s - loss: 0.5824 - acc: 0.8047\n",
      "Epoch 6/30\n",
      "134/134 - 116s - loss: 0.5146 - acc: 0.8313\n",
      "Epoch 7/30\n",
      "134/134 - 117s - loss: 0.4547 - acc: 0.8510\n",
      "Epoch 8/30\n",
      "134/134 - 116s - loss: 0.4124 - acc: 0.8699\n",
      "Epoch 9/30\n",
      "134/134 - 118s - loss: 0.3701 - acc: 0.8802\n",
      "Epoch 10/30\n",
      "134/134 - 117s - loss: 0.3423 - acc: 0.8915\n",
      "Epoch 11/30\n",
      "134/134 - 116s - loss: 0.3230 - acc: 0.8988\n",
      "Epoch 12/30\n",
      "134/134 - 116s - loss: 0.3005 - acc: 0.9020\n",
      "Epoch 13/30\n",
      "134/134 - 117s - loss: 0.2752 - acc: 0.9105\n",
      "Epoch 14/30\n",
      "134/134 - 115s - loss: 0.2653 - acc: 0.9174\n",
      "Epoch 15/30\n",
      "134/134 - 118s - loss: 0.2528 - acc: 0.9184\n",
      "Epoch 16/30\n",
      "134/134 - 118s - loss: 0.2479 - acc: 0.9209\n",
      "Epoch 17/30\n",
      "134/134 - 117s - loss: 0.2370 - acc: 0.9244\n",
      "Epoch 18/30\n",
      "134/134 - 116s - loss: 0.2329 - acc: 0.9265\n",
      "Epoch 19/30\n",
      "134/134 - 114s - loss: 0.2085 - acc: 0.9305\n",
      "Epoch 20/30\n",
      "134/134 - 115s - loss: 0.2099 - acc: 0.9356\n",
      "Epoch 21/30\n",
      "134/134 - 86s - loss: 0.2062 - acc: 0.9362\n",
      "Epoch 22/30\n",
      "134/134 - 78s - loss: 0.1971 - acc: 0.9406\n",
      "Epoch 23/30\n",
      "134/134 - 80s - loss: 0.1993 - acc: 0.9395\n",
      "Epoch 24/30\n",
      "134/134 - 79s - loss: 0.1925 - acc: 0.9397\n",
      "Epoch 25/30\n",
      "134/134 - 79s - loss: 0.1932 - acc: 0.9382\n",
      "Epoch 26/30\n",
      "134/134 - 79s - loss: 0.1891 - acc: 0.9413\n",
      "Epoch 27/30\n",
      "134/134 - 78s - loss: 0.1776 - acc: 0.9455\n",
      "Epoch 28/30\n",
      "134/134 - 78s - loss: 0.1802 - acc: 0.9433\n",
      "Epoch 29/30\n",
      "134/134 - 84s - loss: 0.1744 - acc: 0.9479\n",
      "Epoch 30/30\n",
      "134/134 - 82s - loss: 0.1820 - acc: 0.9472\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x16d3e180ca0>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognizer.fit_generator(datagen.flow(train_data,train_label, batch_size=100),\n",
    "                             epochs = 30, verbose = 2, steps_per_epoch=train_data.shape[0] // 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = recognizer.predict(test_data)\n",
    "predictions = np.argmax(predictions,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(test_label, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.9642857142857143\n"
     ]
    }
   ],
   "source": [
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = sum(cm[i][i] for i in range(28)) / test_label.shape[0]\n",
    "print(\"accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[120   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0 120   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0 117   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   1   0   0   0]\n",
      " [  0   0   4 113   0   0   0   0   0   0   0   0   0   0   1   0   0   0\n",
      "    0   0   0   1   0   0   1   0   0   0]\n",
      " [  0   0   0   0 120   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   2 118   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   1 118   0   0   0   0   0   0   0   0   0   0   0\n",
      "    1   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 113   2   3   0   0   0   0   0   1   0   0\n",
      "    0   0   0   0   0   0   0   0   1   0]\n",
      " [  0   0   0   0   0   0   0   4 107   0   2   0   0   0   1   2   0   0\n",
      "    0   1   0   0   2   0   0   0   1   0]\n",
      " [  1   0   0   0   0   0   0   1   0 114   4   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  1   0   0   0   0   0   0   0   5   6 108   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 117   0   3   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0 119   0   1   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   1   0 118   1   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   2   3 115   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 119   0   0\n",
      "    0   0   0   0   0   0   0   1   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   4 115   0\n",
      "    0   0   1   0   0   0   0   0   0   0]\n",
      " [  1   0   0   0   0   2   2   0   0   0   0   0   0   0   0   0   1 114\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0\n",
      "  119   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   1   0   0\n",
      "    0 115   3   0   0   0   0   0   0   0]\n",
      " [  0   0   1   0   0   0   0   0   0   0   0   0   1   1   0   0   0   0\n",
      "    0   2 115   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0\n",
      "    0   0   1 117   0   0   0   0   1   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   2 117   0   1   0   0   0]\n",
      " [  2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0 117   0   1   0   0]\n",
      " [  0   0   6   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   1   0   4   0   0 107   0   0   0]\n",
      " [  0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0 116   3   0]\n",
      " [  0   0   0   0   0   0   0   2   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   3 115   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0\n",
      "    0   0   1   0   0   0   1   0   0 117]]\n"
     ]
    }
   ],
   "source": [
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Enter the alpha value [1.0-3.0]: 1.5\n",
      "* Enter the beta value [0-100]: 0\n",
      "179.0\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Read image given by user\n",
    "\n",
    "\n",
    "Tk().withdraw() # we don't want a full GUI, so keep the root window from appearing\n",
    "filename = askopenfilename() # show an \"Open\" dialog box and return the path to the selected file\n",
    "image = cv2.imread(filename)#r\"C:\\Users\\rassa\\Desktop\\Desktop2.0\\CNN\\9alb-Ba2-0.png\") #(cv.samples.findFile(args.input))\n",
    "\n",
    "contrast_img = np.zeros(image.shape, image.dtype)\n",
    "\n",
    "alpha = 1.0 # Simple contrast control\n",
    "beta = 0    # Simple brightness control\n",
    "\n",
    "# Initialize values\n",
    "try:\n",
    "    alpha = float(input('* Enter the alpha value [1.0-3.0]: '))\n",
    "    beta = int(input('* Enter the beta value [0-100]: '))\n",
    "except ValueError:\n",
    "    print('Error, not a number')\n",
    "    \n",
    "# Do the operation new_image(i,j) = alpha*image(i,j) + beta\n",
    "# Instead of these 'for' loops we could have used simply:\n",
    "# new_image = cv.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
    "# but we wanted to show you how to access the pixels :)\n",
    "for y in range(image.shape[0]):\n",
    "    for x in range(image.shape[1]):\n",
    "        for c in range(image.shape[2]):\n",
    "            # the clip makes sure that the new values of the pixel are between 0 and 255\n",
    "            contrast_img[y,x,c] = np.clip(alpha*image[y,x,c] + beta, 0, 255)\n",
    "        \n",
    "#removethe three dimensions and turn image to gray scale\n",
    "im_gray = cv2.cvtColor(contrast_img, cv2.COLOR_BGR2GRAY)\n",
    "#binarize the image using Otsu method to get the threshold, binarize to black and white(ie. 255)\n",
    "th, im_gray_th_otsu = cv2.threshold(im_gray, 128, 255, cv2.THRESH_OTSU)\n",
    "#resizing...\n",
    "resized = cv2.resize(im_gray_th_otsu, (32,32), interpolation = cv2.INTER_AREA)\n",
    "\n",
    "#print the threshold because I'm curious\n",
    "print(th)\n",
    "\n",
    "final_img = np.zeros(resized.shape, resized.dtype)\n",
    "for y in range(resized.shape[0]):\n",
    "    for x in range(resized.shape[1]):\n",
    "        if resized[y,x] < 125:     \n",
    "            final_img[y,x] = 255\n",
    "        else :\n",
    "            final_img[y,x] = 0\n",
    "\n",
    "#Show the images so we can compare them (use the cv2.imshow if you want to ee them all at once)\n",
    "#cv2.imshow('New Image', im_gray_th_otsu)\n",
    "#cv2.imshow('Original Image', image)\n",
    "#cv2.imshow('Contrast image', contrast_img)\n",
    "#cv2.imshow('resized Image', resized)\n",
    "#cv2.imshow('final Image', final_img)\n",
    "plt.imshow(image)\n",
    "plt.imshow(contrast_img)\n",
    "plt.imshow(im_gray_th_otsu, cmap='gray', vmin=0, vmax=255)\n",
    "plt.imshow(resized, cmap='gray', vmin=0, vmax=255)\n",
    "plt.imshow(final_img, cmap='gray', vmin=0, vmax=255)\n",
    "\n",
    "\n",
    "final_img = np.asarray(final_img).reshape([-1, 32, 32, 1])\n",
    "#final_img.reshape([1,32,32,1])\n",
    "#final_img = np.expand_dims(final_img, -1)\n",
    "\n",
    "predictions_single = recognizer.predict(final_img)\n",
    "print(predictions_single)\n",
    "\n",
    "#let's get our label\n",
    "label = 0\n",
    "for i in range(28):\n",
    "    if predictions_single[0][i] == 1:\n",
    "        label = i+1\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here I tried to create a simple GUI but I didn't have enough time to do it\n",
    "\n",
    "def outputMsg:\n",
    "    messagebox.showinfo(\"Prediction\", \"Your letter is \" + label)\n",
    "\n",
    "    def switch():\n",
    "   \n",
    "    option = int(label)\n",
    " \n",
    "    def alif():\n",
    "        message = \"Alif\"\n",
    "    def ba():\n",
    "        message = \"ba2\"\n",
    "    def ta():\n",
    "        message = \"ta2\"\n",
    "    def tha():\n",
    "        message = \"tha2\"\n",
    "    def jha():\n",
    "        message = \"jha2\"\n",
    "    def ha():\n",
    "        message = \"7a2\"\n",
    "    def kha():\n",
    "        message = \"5a2\"\n",
    "    def da():\n",
    "        message = \"da2\"\n",
    "    def ra():\n",
    "        message = \"ra2\"\n",
    "    def za():\n",
    "        message = \"za2\"\n",
    "    def thal():\n",
    "        message = \"thal\"\n",
    "    def sa():\n",
    "        message = \"sa2\"\n",
    "    def sha():\n",
    "        message = \"sha2\"\n",
    "    def sad():\n",
    "        message = \"saad\"\n",
    "    def thad():\n",
    "        message = \"thad\"\n",
    "    def tta():\n",
    "        message = \"tta2\"\n",
    "    def ththa():\n",
    "        message = \"ththa2\"\n",
    "    def aayn():\n",
    "        message = \"3ayn\"\n",
    "    def ghayn():\n",
    "        message = \"ghayn\"\n",
    "    def fa():\n",
    "        message = \"fa2\"\n",
    "    def kka():\n",
    "        message = \"9a2\"\n",
    "    def ka():\n",
    "        message = \"ka2\"\n",
    "    def lam():\n",
    "        message = \"lam\"\n",
    "    def meem():\n",
    "        message = \"meem\"\n",
    "    def noon():\n",
    "        message = \"noon\"\n",
    "    def haa():\n",
    "        message = \"haa2\"\n",
    "    def waw():\n",
    "        message = \"waw\"\n",
    "    def yaa():\n",
    "        message = \"yaa2\"\n",
    "        \n",
    "    def default():\n",
    "        message = \"Something Went Wrong!\"\n",
    " \n",
    "    dict = {\n",
    "        1 : alif,\n",
    "        2 : ba,\n",
    "        3: ta\n",
    "        4: tha\n",
    "        5: jha\n",
    "        6: ha,\n",
    "        7: kha,\n",
    "        8: da,\n",
    "        9: thal,\n",
    "        10: ra \n",
    "        11: za \n",
    "        12: sa, \n",
    "        13: sha,\n",
    "        14: sad,\n",
    "        15: thad,\n",
    "        16: tta,\n",
    "        17: ththa,\n",
    "        18: aayn,\n",
    "        19: ghayn,\n",
    "        20: fa,\n",
    "        21: kka,\n",
    "        22: ka,\n",
    "        23: lam,\n",
    "        24: meem,\n",
    "        25: noon,\n",
    "        26: haa,\n",
    "        27: waw,\n",
    "        28: yaa\n",
    " \n",
    "    }\n",
    "    dict.get(option,default)() # get() method returns the function matching the argument\n",
    " \n",
    " \n",
    "switch()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
