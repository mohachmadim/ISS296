{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rassa\\anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import tflearn.data_utils as du\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.preprocessing.image import load_img \n",
    "from keras.preprocessing.image import img_to_array \n",
    "from keras.preprocessing.image import array_to_img\n",
    "import warnings\n",
    "import matplotlib.image as mpimg\n",
    "import os \n",
    "import cv2\n",
    "import tensorflow as tf \n",
    "from PIL import Image \n",
    "from numpy import asarray\n",
    "from tqdm import tqdm\n",
    "from tkinter import Tk     # from tkinter import Tk for Python 3.x\n",
    "from tkinter.filedialog import askopenfilename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 13440/13440 [00:10<00:00, 1254.28it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 3360/3360 [00:02<00:00, 1331.09it/s]\n"
     ]
    }
   ],
   "source": [
    "DIRECTORY = r\"C:\\Users\\rassa\\Documents\\Machine Learning A-Z  Udemy\\Arabic Handwritten Characters Dataset\"\n",
    "train_data = []\n",
    "test_data = []\n",
    "trainPath = DIRECTORY + r\"\\Train Images 13440x32x32\\train\"\n",
    "testPath  = DIRECTORY + r\"\\Test Images 3360x32x32\\test\"\n",
    "\n",
    "def getImgId(imgName):\n",
    "    return int(imgName.split('_')[1])\n",
    "\n",
    "def makeData(data, directory, sortingCriteria):\n",
    "    imgFileNameList = os.listdir(directory)     #get list of directories in path (ie. list of image names)\n",
    "    imgFileNameList.sort(key = sortingCriteria) #sort list based on key\n",
    "    \n",
    "    for img in tqdm(imgFileNameList):  # iterate over the sorted list\n",
    "                try:\n",
    "                    img_array = cv2.imread(os.path.join(directory, img) ,cv2.IMREAD_GRAYSCALE)  # convert image to array\n",
    "                    data.append([img_array])  # add this to our data\n",
    "                except Exception as e:  # just in case there is a mistake.\n",
    "                    pass\n",
    "\n",
    "makeData(train_data, trainPath, getImgId)\n",
    "makeData(test_data, testPath, getImgId)\n",
    "\n",
    "# convert to ndarray and reshape\n",
    "train_data = np.asarray(train_data).reshape([-1, 32, 32, 1])\n",
    "test_data = np.asarray(test_data).reshape([-1, 32, 32, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the labels\n",
    "#header = 0 because\n",
    "#this creates a pandas dataframe (basically a table)\n",
    "train_label = pd.read_csv(DIRECTORY + '/csvTrainLabel 13440x1.csv', header = None)\n",
    "test_label = pd.read_csv(DIRECTORY +  '/csvTestLabel 3360x1.csv', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data = train_data.iloc[:,:].values.astype('float32')\n",
    "train_label = train_label.iloc[:,:].values.astype('int32')-1\n",
    "#test_data = test_data.iloc[:,:].values.astype('float32') Not sure if we need this yet\n",
    "test_label = test_label.iloc[:,:].values.astype('int32')-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = du.to_categorical(train_label,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data/255\n",
    "test_data = test_data/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data, mean1 = du.featurewise_zero_center(train_data)\n",
    "#test_data, mean2 = du.featurewise_zero_center(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "recognizer = Sequential()\n",
    "#for any kind of deep learning we use the sequential model in keras then \n",
    "#add layers to it\n",
    "recognizer.add(Conv2D(filters = 18, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu', input_shape = (32,32,1)))\n",
    "#the first 2 dimensional convolusion layer will have 32 filters. filters or kernels are \n",
    "#what we use to extract features. in this case they are of size 5x5 (kernel size)\n",
    "#in the first layer it is necessary to specify the shape so for our case its a\n",
    "#32x32 pixel image and since it's black and white then it has only one dimension\n",
    "#if it was colored then we would have (32,32,3)\n",
    "#activation='relu' this is rectified linear unit. the output filters or convolced layers\n",
    "#might contain some negative values so we apply the rectifier function (or other functions)\n",
    "recognizer.add(Conv2D(filters = 18, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "recognizer.add(MaxPool2D(pool_size=(2,2)))\n",
    "recognizer.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "recognizer.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "recognizer.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "recognizer.add(MaxPool2D(pool_size=(2,2), strides=(1,1)))\n",
    "recognizer.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "recognizer.add(Flatten())\n",
    "recognizer.add(Dense(units = 256, input_dim = 1024, activation = 'relu'))\n",
    "recognizer.add(Dense(units = 256, activation = \"relu\"))\n",
    "recognizer.add(Dropout(0.5))\n",
    "recognizer.add(Dense(28, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 18)        180       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 18)        2934      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 18)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16, 16, 18)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 32)        5216      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 7200)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               1843456   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 28)                7196      \n",
      "=================================================================\n",
      "Total params: 1,934,022\n",
      "Trainable params: 1,934,022\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "recognizer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "recognizer.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False, \n",
    "        samplewise_center=False,  \n",
    "        featurewise_std_normalization=False,\n",
    "        samplewise_std_normalization=False,\n",
    "        zca_whitening=False,\n",
    "        rotation_range=10,\n",
    "        zoom_range = 0.1,  \n",
    "        width_shift_range=0.1, \n",
    "        height_shift_range=0.1,\n",
    "        horizontal_flip=False,\n",
    "        vertical_flip=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rassa\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py:1240: UserWarning: `model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "134/134 - 52s - loss: 2.5142 - acc: 0.2478\n",
      "Epoch 2/30\n",
      "134/134 - 53s - loss: 1.4735 - acc: 0.5129\n",
      "Epoch 3/30\n",
      "134/134 - 61s - loss: 1.0710 - acc: 0.6472\n",
      "Epoch 4/30\n",
      "134/134 - 74s - loss: 0.8507 - acc: 0.7142\n",
      "Epoch 5/30\n",
      "134/134 - 71s - loss: 0.7289 - acc: 0.7613\n",
      "Epoch 6/30\n",
      "134/134 - 71s - loss: 0.6206 - acc: 0.7968\n",
      "Epoch 7/30\n",
      "134/134 - 73s - loss: 0.5582 - acc: 0.8167\n",
      "Epoch 8/30\n",
      "134/134 - 74s - loss: 0.5052 - acc: 0.8371\n",
      "Epoch 9/30\n",
      "134/134 - 72s - loss: 0.4596 - acc: 0.8516\n",
      "Epoch 10/30\n",
      "134/134 - 74s - loss: 0.4326 - acc: 0.8582\n",
      "Epoch 11/30\n",
      "134/134 - 74s - loss: 0.3998 - acc: 0.8691\n",
      "Epoch 12/30\n",
      "134/134 - 71s - loss: 0.3771 - acc: 0.8741\n",
      "Epoch 13/30\n",
      "134/134 - 75s - loss: 0.3594 - acc: 0.8811\n",
      "Epoch 14/30\n",
      "134/134 - 71s - loss: 0.3305 - acc: 0.8918\n",
      "Epoch 15/30\n",
      "134/134 - 73s - loss: 0.3179 - acc: 0.8972\n",
      "Epoch 16/30\n",
      "134/134 - 71s - loss: 0.3088 - acc: 0.8995\n",
      "Epoch 17/30\n",
      "134/134 - 71s - loss: 0.3041 - acc: 0.9035\n",
      "Epoch 18/30\n",
      "134/134 - 72s - loss: 0.2924 - acc: 0.9073\n",
      "Epoch 19/30\n",
      "134/134 - 70s - loss: 0.2679 - acc: 0.9140\n",
      "Epoch 20/30\n",
      "134/134 - 72s - loss: 0.2706 - acc: 0.9147\n",
      "Epoch 21/30\n",
      "134/134 - 71s - loss: 0.2580 - acc: 0.9142\n",
      "Epoch 22/30\n",
      "134/134 - 71s - loss: 0.2623 - acc: 0.9147\n",
      "Epoch 23/30\n",
      "134/134 - 71s - loss: 0.2510 - acc: 0.9223\n",
      "Epoch 24/30\n",
      "134/134 - 70s - loss: 0.2431 - acc: 0.9205\n",
      "Epoch 25/30\n",
      "134/134 - 71s - loss: 0.2431 - acc: 0.9238\n",
      "Epoch 26/30\n",
      "134/134 - 72s - loss: 0.2329 - acc: 0.9282\n",
      "Epoch 27/30\n",
      "134/134 - 71s - loss: 0.2271 - acc: 0.9287\n",
      "Epoch 28/30\n",
      "134/134 - 70s - loss: 0.2242 - acc: 0.9282\n",
      "Epoch 29/30\n",
      "134/134 - 72s - loss: 0.2234 - acc: 0.9296\n",
      "Epoch 30/30\n",
      "134/134 - 74s - loss: 0.2092 - acc: 0.9325\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2ae76e16100>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognizer.fit_generator(datagen.flow(train_data,train_label, batch_size=100),\n",
    "                             epochs = 30, verbose = 2, steps_per_epoch=train_data.shape[0] // 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rassa\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    }
   ],
   "source": [
    "predictions = recognizer.predict(test_data)\n",
    "predictions = np.argmax(predictions,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(test_label, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.9586309523809524\n"
     ]
    }
   ],
   "source": [
    "accuracy = sum(cm[i][i] for i in range(28)) / test_label.shape[0]\n",
    "print(\"accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[120   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0 119   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0 117   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   1   0   0   1   0   0   0]\n",
      " [  0   0   4 113   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   1   0   0   2   0   0   0]\n",
      " [  0   0   0   1 118   0   0   0   0   0   0   0   0   0   0   0   0   1\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   2 116   0   0   0   0   0   0   0   0   0   0   0   2\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   1 119   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   1   0 115   0   3   0   0   0   0   0   1   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   6 106   0   1   0   0   0   0   3   0   0\n",
      "    0   0   0   0   2   0   1   0   1   0]\n",
      " [  0   0   0   0   0   0   0   2   0 115   2   0   0   0   0   0   0   0\n",
      "    0   0   0   0   1   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   1   6  11 101   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   1   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 117   1   2   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   1   0   0   0   0   0   0   0   0 119   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   2   0 118   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   1   0   0   0   0   0   0   0   0   1   3 115   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 115   3   0\n",
      "    0   0   0   0   0   0   0   2   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   5 115   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   3   0   0   0   0   0   0   0   0   1   0 115\n",
      "    0   0   0   0   0   1   0   0   0   0]\n",
      " [  0   0   0   0   0   0   5   0   0   0   0   0   0   0   0   0   0   1\n",
      "  114   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   1   0   0   0   0   0   0   0   0   0   0   1   1   0   0\n",
      "    0 112   4   0   0   0   1   0   0   0]\n",
      " [  0   0   1   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0\n",
      "    0   4 113   0   0   0   0   0   0   1]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0 119   0   0   0   0   1   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   2 117   0   1   0   0   0]\n",
      " [  1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0 117   0   2   0   0]\n",
      " [  0   0   8   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   1   0   4   0   0 106   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0 117   3   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   5 115   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0\n",
      "    0   0   0   0   0   0   1   0   0 118]]\n"
     ]
    }
   ],
   "source": [
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Enter the alpha value [1.0-3.0]: 1.5\n",
      "* Enter the beta value [0-100]: 0\n",
      "230.0\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 1.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Read image given by user\n",
    "\n",
    "\n",
    "#Tk().withdraw() # we don't want a full GUI, so keep the root window from appearing\n",
    "#filename = askopenfilename() # show an \"Open\" dialog box and return the path to the selected file\n",
    "image = cv2.imread(r\"C:\\Users\\rassa\\Desktop\\Desktop2.0\\CNN\\9alb-Ba2-0.png\") #(cv.samples.findFile(args.input))\n",
    "\n",
    "contrast_img = np.zeros(image.shape, image.dtype)\n",
    "\n",
    "alpha = 1.0 # Simple contrast control\n",
    "beta = 0    # Simple brightness control\n",
    "\n",
    "# Initialize values\n",
    "try:\n",
    "    alpha = float(input('* Enter the alpha value [1.0-3.0]: '))\n",
    "    beta = int(input('* Enter the beta value [0-100]: '))\n",
    "except ValueError:\n",
    "    print('Error, not a number')\n",
    "    \n",
    "# Do the operation new_image(i,j) = alpha*image(i,j) + beta\n",
    "# Instead of these 'for' loops we could have used simply:\n",
    "# new_image = cv.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
    "# but we wanted to show you how to access the pixels :)\n",
    "for y in range(image.shape[0]):\n",
    "    for x in range(image.shape[1]):\n",
    "        for c in range(image.shape[2]):\n",
    "            # the clip makes sure that the new values of the pixel are between 0 and 255\n",
    "            contrast_img[y,x,c] = np.clip(alpha*image[y,x,c] + beta, 0, 255)\n",
    "        \n",
    "#removethe three dimensions and turn image to gray scale\n",
    "im_gray = cv2.cvtColor(contrast_img, cv2.COLOR_BGR2GRAY)\n",
    "#binarize the image using Otsu method to get the threshold, binarize to black and white(ie. 255)\n",
    "th, im_gray_th_otsu = cv2.threshold(im_gray, 128, 255, cv2.THRESH_OTSU)\n",
    "\n",
    "resized = cv2.resize(im_gray_th_otsu, (32,32), interpolation = cv2.INTER_AREA)\n",
    "\n",
    "#print the threshold because I'm curious\n",
    "print(th)\n",
    "\n",
    "final_img = np.zeros(resized.shape, resized.dtype)\n",
    "for y in range(resized.shape[0]):\n",
    "    for x in range(resized.shape[1]):\n",
    "        if resized[y,x] < 125:     \n",
    "            final_img[y,x] = 255\n",
    "        else :\n",
    "            final_img[y,x] = 0\n",
    "\n",
    "#Show the images so we can compare them\n",
    "#cv2.imshow('New Image', im_gray_th_otsu)\n",
    "#cv2.imshow('Original Image', image)\n",
    "#cv2.imshow('Contrast image', contrast_img)\n",
    "#cv2.imshow('resized Image', resized)\n",
    "#cv2.imshow('final Image', final_img)\n",
    "plt.imshow(image)\n",
    "plt.imshow(contrast_img)\n",
    "plt.imshow(im_gray_th_otsu, cmap='gray', vmin=0, vmax=255)\n",
    "plt.imshow(resized, cmap='gray', vmin=0, vmax=255)\n",
    "plt.imshow(final_img, cmap='gray', vmin=0, vmax=255)\n",
    "\n",
    "\n",
    "final_img = np.asarray(final_img).reshape([-1, 32, 32, 1])\n",
    "#final_img.reshape([1,32,32,1])\n",
    "#final_img = np.expand_dims(final_img, -1)\n",
    "\n",
    "predictions_single = recognizer.predict(final_img)\n",
    "print(predictions_single)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
